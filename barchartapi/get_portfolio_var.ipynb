{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_portfolio_var - \n",
    "1. Use the barchart api to retrieve stock historical data;\n",
    "2. Create a correlation matrix from the historical data; \n",
    "3. Get standard deviations from the historical data;\n",
    "4. Create long/short position information for each security retrieved\n",
    "5. Create a portolio VaR using all of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from os.path import expanduser\n",
    "from scipy.stats import norm\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to sys.path\n",
    "Within the barchart_api module, references like: ```from barchartapi import barchart_api``` will not work unless you append to sys.path the following folders:\n",
    "1. project working directory (which holds barchart_api.py),\n",
    "2. project directory,\n",
    "3. workspace directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_DIR = '../..' \n",
    "PROJECT_DIR = f'{WORKSPACE_DIR}/barchartapi' \n",
    "WORKING_DIR = f'{PROJECT_DIR}/barchartapi' \n",
    "if WORKING_DIR not in sys.path:\n",
    "    sys.path.append(WORKING_DIR)\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    sys.path.append(PROJECT_DIR)\n",
    "if WORKSPACE_DIR not in sys.path:\n",
    "    sys.path.append(WORKSPACE_DIR)\n",
    "import barchart_api as bcapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create paths to where you will store the history data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCKS_DIR = f'./temp_folder/stocks'\n",
    "try:\n",
    "    os.makedirs(STOCKS_DIR)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create an instance of BcHist\n",
    "1. must set bar_type to minutes, hour or daily\n",
    "2. must set interval\n",
    " * for minutes, set interval to 1, 5, 15, 30 or 60\n",
    " * for other types, see https://www.barchart.com/ondemand/api/getHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set this to 'free' or 'paid'\n",
    "endpoint = 'free' # free or paid\n",
    "\n",
    "# set the bar_type and the interval\n",
    "bar_type='daily' # minutes, daily, monthly\n",
    "interval=1 # 1,5,15,30,60\n",
    "\n",
    "# create an instance \n",
    "api_key = open(f'./temp_folder/{endpoint}_api_key.txt','r').read()\n",
    "endpoint_type=f'{endpoint}_url'\n",
    "bch = bcapi.BcHist(api_key, bar_type=bar_type, interval=interval,endpoint_type = endpoint_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set delete_old_csv_file to True if you want to re-retrieve the csv files that we use in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true will cause barchartapi to refetch csv data, \n",
    "#   false will cause it to bypass any short_name that already has a csv file in the path sn_path below\n",
    "delete_old_csv_file = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method to read your csv portfolio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a method to retrieve the portfolio\n",
    "def get_portfolio(csv_path=None):\n",
    "    '''\n",
    "    fetch a portfolio to use below\n",
    "    '''\n",
    "    p = csv_path\n",
    "    if p is None:\n",
    "        p = './hi_volume_stocks.csv'\n",
    "    df_p = pd.read_csv(p)\n",
    "    df_p['under'] = df_p.symbol.apply(lambda s: s.split(\"_\")[0])    \n",
    "    pnames = list(df_p.under)\n",
    "    return (df_p,pnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your portfolio\n",
    "1. Enter **portfolio_folder_to_search** : a folder to search for a portfolio csv\n",
    "2. Enter **portfolio_csv_name** : a csv file name in that folder that has 2 columns:\n",
    " * symbol column\n",
    " * position column\n",
    " *   \n",
    "\n",
    "3. (default is ./hi_volume_stocks.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter a folder and a name\n",
    "portfolio_folder_to_search = '.'\n",
    "portfolio_csv_name = 'fxe_portfolio.csv'\n",
    "\n",
    "csv_file = portfolio_folder_to_search + \"/\" + portfolio_csv_name\n",
    "print('possible csvs: ',list(filter(lambda d: '.csv' in str(d).lower(),os.listdir(portfolio_folder_to_search))))\n",
    "p_tuple = get_portfolio(csv_file)\n",
    "df_portfolio = p_tuple[0]\n",
    "print('names in portfolio',p_tuple[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Main Loop to fetch history data from Barchart\n",
    "\n",
    "#### Inputs\n",
    "On each call to BcHist.get_history, provide:\n",
    " * a beginning yyyymmdd integer\n",
    " * an ending yyyymmdd integer\n",
    " * a short name like:\n",
    "  * a stock symbol like: SPY,USO,IBM, etc\n",
    "  * a futures symbol like: CLJ18, GCG19, ESH17, etc\n",
    "\n",
    "#### Outputs\n",
    "The get_history method will return a tuple, where:\n",
    " * tup[0] provides a status\n",
    "\n",
    "* tup[1] provides a pandas DataFrame of data, or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  set a date range\n",
    "days_to_fetch = 120\n",
    "dt_end = datetime.datetime.now()\n",
    "dt_beg = dt_end - datetime.timedelta(days_to_fetch)\n",
    "beg_yyyymmdd = '%04d%02d%02d' %(dt_beg.year,dt_beg.month,dt_beg.day)#20181201\n",
    "end_yyyymmdd = '%04d%02d%02d' %(dt_end.year,dt_end.month,dt_end.day)#20190219\n",
    "\n",
    "\n",
    "# set up other ETF names that can be used as a hedge to the portfolio\n",
    "spydr_short_names = ['XLE','XLU','XLK','XLB','XLP','XLY','XLI','XLC','XLV','XLF']\n",
    "equity_etf_names = ['DIA','SPY','QQQ']\n",
    "commodity_etf_short_names = ['USO','UNG','DBC','DBA','GLD','USCI']\n",
    "currency_etf_short_names = ['FXY','FXE','FXB','FXF','FXC','FXA']\n",
    "all_names = spydr_short_names + equity_etf_names + commodity_etf_short_names + currency_etf_short_names + commodity_etf_short_names\n",
    "\n",
    "# get the portfolio\n",
    "port_names = p_tuple[1]\n",
    "\n",
    "# set short_names to the desired list to get\n",
    "short_names = list(set(\n",
    "    port_names + spydr_short_names + ['SPY']))\n",
    "\n",
    "for short_name in short_names:\n",
    "    sn_path = f'{STOCKS_DIR}/{short_name}.csv'\n",
    "    if os.path.isfile(sn_path):\n",
    "        if delete_old_csv_file:\n",
    "            os.remove(sn_path)\n",
    "        else:\n",
    "            print(f'BYPASSING: {short_name}')\n",
    "            continue\n",
    "    print(f'get_history: {short_name} BEGIN {datetime.datetime.now()}')\n",
    "    tup = bch.get_history(short_name, beg_yyyymmdd, end_yyyymmdd)\n",
    "    print(f'get_history: {short_name} WRITING DATA {datetime.datetime.now()}')\n",
    "    tup[1].to_csv(sn_path,index=False)\n",
    "    print(f'get_history: {short_name} END {datetime.datetime.now()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Do something interesting with the returned data, like create a portfolio VaR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First create correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_of_std = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_hist= None\n",
    "df_hist = None\n",
    "for short_name in short_names:\n",
    "    sn_path = f'{STOCKS_DIR}/{short_name}.csv'\n",
    "    df_temp = pd.read_csv(sn_path).iloc[-1*days_of_std:]\n",
    "    if df_all_hist is None:\n",
    "        df_all_hist = df_temp.copy()\n",
    "    else:\n",
    "        df_all_hist = df_all_hist.append(df_temp)\n",
    "        df_all_hist.index = list(range(len(df_all_hist)))\n",
    "    df_temp = df_temp[['tradingDay','close']]\n",
    "    df_temp = df_temp.rename(columns={'close':f'{short_name}'})\n",
    "    if df_hist is None:\n",
    "        df_hist = df_temp.copy()\n",
    "    else:\n",
    "        df_hist = df_hist.merge(df_temp,how='inner',on='tradingDay')\n",
    "df_corr = df_hist[df_portfolio.under].corr()\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next create standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(set(list(df_hist.columns.values))-set(['tradingDay']))\n",
    "bars_per_day = 1\n",
    "if bar_type.lower()!='daily':\n",
    "    if bar_type.lower()=='minutes':\n",
    "        bars_per_day = 8*2\n",
    "perc_of_day = 1/bars_per_day\n",
    "perc_of_year = perc_of_day/256\n",
    "std_series = df_hist[cols].pct_change().iloc[1:].std()/perc_of_year**.5\n",
    "df_std = pd.DataFrame({'stdev':list(std_series.values),'symbol':list(std_series.index.values)})\n",
    "df_std.sort_values('symbol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positions = df_portfolio[['under','position']].rename(columns={'under':'symbol'})\n",
    "df_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get current prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(filter(lambda c:'time' not in c,df_hist.columns.values))\n",
    "vals = df_hist.iloc[-1:][cols].values.reshape(-1)\n",
    "df_prices = pd.DataFrame({'symbol':cols,'price':vals})[['symbol','price']]\n",
    "df_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge position, prices, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positions_2 = df_positions.merge(df_prices,how='inner',on='symbol')\n",
    "df_positions_3 = df_positions_2.merge(df_std,how='inner',on='symbol')\n",
    "df_positions_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create position VaR's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_CONFIDENCE = .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positions_3['unit_var'] = df_positions_3.apply(lambda r: r.price * r.stdev * norm.ppf(VAR_CONFIDENCE) * (1/256)**.5 / r.price,axis=1 )\n",
    "df_positions_3['position_var'] = df_positions_3.apply(lambda r: r.unit_var * r.position * r.price ,axis=1 )\n",
    "df_positions_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create portfolio VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an spy standard deviation that is the historical average\n",
    "var_days = 1\n",
    "spy_usual_daily_std = .16 \n",
    "port_std = (df_positions_3.position_var.astype(float).values.T @ df_corr.astype(float).values @ df_positions_3.position_var.astype(float).values)**.5\n",
    "port_var = port_std * var_days**.5  \n",
    "spy_curr_unit_var = float(df_std[df_std.symbol=='SPY'].stdev) * (var_days/256)**.5  * norm.ppf(VAR_CONFIDENCE) \n",
    "sp_dollar_equiv = port_var / spy_curr_unit_var \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'portolio VaR: {round(port_var,2)}')\n",
    "print(f\"sp {var_days} day{'s' if var_days>1 else ''} unit VaR: {round(spy_curr_unit_var,2)}\")\n",
    "print(f'Equivalent S&P position (in dollars): {round(sp_dollar_equiv,2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do High Low Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_array = []\n",
    "h5_array = []\n",
    "h15_array = []\n",
    "h20_array = []\n",
    "names = list(set(df_all_hist.symbol.sort_values()))\n",
    "for sym  in names:\n",
    "    df_this = df_all_hist[df_all_hist.symbol==sym]\n",
    "    hl = (df_this.high-df_this.low).sort_values(ascending=False)[:5].mean()/df_this.close[-6:].mean()\n",
    "    hl_array.append(hl)\n",
    "    h5 = (df_this.high.rolling(5).max() -df_this.low.rolling(5).min()).sort_values(ascending=False)[:5].mean()/df_this.close[-6:].mean()\n",
    "    h5_array.append(h5)\n",
    "    h15 = (df_this.high.rolling(15).max() -df_this.low.rolling(15).min()).sort_values(ascending=False)[:10].mean()/df_this.close[-6:].mean()\n",
    "    h15_array.append(h15)\n",
    "    h20 = (df_this.high.rolling(20).max() -df_this.low.rolling(20).min()).sort_values(ascending=False)[:10].mean()/df_this.close[-6:].mean()\n",
    "    h20_array.append(h20)\n",
    "df_high_low = pd.DataFrame({'symbol':names,'h1':hl_array,'h5':h5_array,'h15':h15_array,'h20':h20_array})[['symbol','h1','h5','h15','h20']]\n",
    "df_high_low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Barchart data to Yahoo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as pdr\n",
    "t2 = datetime.datetime.now()\n",
    "t1 = t2 - datetime.timedelta(120)\n",
    "d1 = pdr.DataReader('CORN', 'yahoo', t1, t2)\n",
    "d2 = pd.read_csv(f'{STOCKS_DIR}/CORN.csv').iloc[-1*days_of_std:].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = d2.copy()\n",
    "df3.index = df3.tradingDay.apply(lambda d: pd.Timestamp(d))\n",
    "df3.index.name = 'Date'\n",
    "newcols = {c:c[0].upper()+c[1:] for c in df3.columns.values}\n",
    "df3 = df3.rename(columns=newcols)\n",
    "df3 = df3[['High','Low','Open','Close','Volume']]\n",
    "df3['Adj Close'] = df3.Close\n",
    "df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df3.index)[0],list(d1.index)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
